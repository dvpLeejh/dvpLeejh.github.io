---
title: "About tensor"
tags:
  - Deep Learning

categories:
    - Tensor
last_modified_at: 2021-10-03

use_math: false

toc: true
toc_sticky: true

---


# í…ì„œì˜ ê¸°ì´ˆ


```python
import tensorflow as tf
import numpy as np
```

Tensors are multi-dimensional arrays with a uniform type (called a dtype). You can see all supported dtypes at tf.dtypes.DType.

If you're familiar with NumPy, tensors are (kind of) like np.arrays.

All tensors are immutable like Python numbers and strings: you can never update the contents of a tensor, only create a new one.

---

í…ì„œëŠ” dtypeì´ë¼ ë¶ˆë¦¬ëŠ” uniform typeì„ ê°€ì§„ ë‹¤ì°¨ì› í–‰ë ¬ì´ë‹¤.
ë§Œì•½ ë„˜íŒŒì´ì— ìµìˆ™í•˜ë‹¤ë©´, í…ì„œëŠ” ë„˜íŒŒì´ì˜ ì¼ì¢…ìœ¼ë¡œ ëŠê»´ì§ˆ ê²ƒì´ë‹¤.

ëª¨ë“  í…ì„œëŠ” íŒŒì´ì¬ì˜ ìˆ«ìì™€ ë¬¸ìì—´ê³¼ ê°™ì´ ë°”ê¿€ìˆ˜ ì—†ë‹¤.
ì¦‰ ìƒˆë¡œ ë§Œë“œëŠ”ê±° ì™¸ì—ëŠ” í…ì„œì˜ ë‚´ìš©ë¬¼ì„ ë°”ê¿€ìˆ˜ ì—†ë‹¤.

# **Basics**


Let's create some basic tensors.

Here is a "scalar" or "rank-0" tensor . A scalar contains a single value, and no "axes".


---



ê°„ë‹¨í•œ í…ì„œë¥¼ ë§Œë“¤ì–´ë³´ì.

ì—¬ê¸°ì— "ìŠ¤ì¹¼ë¼" ë˜ëŠ” "rank-0" í…ì„œ.
ìŠ¤ì¹¼ë¼ëŠ” single valueë¥¼ í¬í•¨í•˜ê³ , "axes"ë¥¼ í¬í•¨í•˜ì§€ ì•ŠëŠ”ë‹¤.


```python
# ê¸°ë³¸ì ìœ¼ë¡œ dtypeì´ int32 í…ì„œê°€ ëœë‹¤.
rank_0_tensor = tf.constant(4)
print(rank_0_tensor)
```

    tf.Tensor(4, shape=(), dtype=int32)
    

A "vector" or "rank-1" tensor is like a list of values. A vector has one axis:


---



"ë²¡í„°" ë˜ëŠ” "rank-1" í…ì„œëŠ” valuesì˜ ë¦¬ìŠ¤íŠ¸ì™€ë„ ê°™ë‹¤.

ë²¡í„°ëŠ” í•˜ë‚˜ì˜ axisë¥¼ ê°–ëŠ”ë‹¤.


```python
# float tensorë¥¼ ë§Œë“¤ì–´ë³´ì.
rank_1_tensor = tf.constant([2.0, 3.0, 4.0])
print(rank_1_tensor)
```

    tf.Tensor([2. 3. 4.], shape=(3,), dtype=float32)
    

A "matrix" or "rank-2" tensor has two axes:


---


"í–‰ë ¬" ë˜ëŠ” "rank-2" í…ì„œëŠ” ë‘ê°œì˜ axesë¥¼ ê°€ì§„ë‹¤.


```python
# If you want to be specific, you can set the dtype (see below) at creation time
# ë§Œì•½ êµ¬ì²´ì ì¸ê±¸ ì›í•œë‹¤ë©´, ìƒì„±ì‹œ dtypeì„ ì„¤ì •í• ìˆ˜ ìˆë‹¤.

rank_2_tensor = tf.constant([[1, 2],
                             [3, 4],
                             [5, 6]], dtype=tf.float16)
print(rank_2_tensor)
```

    tf.Tensor(
    [[1. 2.]
     [3. 4.]
     [5. 6.]], shape=(3, 2), dtype=float16)
    

![ê·¸ë¦¼1](https://user-images.githubusercontent.com/42956142/135858475-984d7228-8b20-4436-a47b-cbb66e8a9f3b.PNG)

Tensors may have more axes; here is a tensor with three axes:


---



í…ì„œëŠ” ë”ë§ì€ axesë¥¼ ê°€ì§ˆìˆ˜ìˆë‹¤.

axesê°€ 3ì¸ í…ì„œë¥¼ ì‚´í´ë³´ì.


```python
# There can be an arbitrary number of
# axes (sometimes called "dimensions")
rank_3_tensor = tf.constant([
  [[0, 1, 2, 3, 4],
   [5, 6, 7, 8, 9]],
  [[10, 11, 12, 13, 14],
   [15, 16, 17, 18, 19]],
  [[20, 21, 22, 23, 24],
   [25, 26, 27, 28, 29]],])

print(rank_3_tensor)
```

    tf.Tensor(
    [[[ 0  1  2  3  4]
      [ 5  6  7  8  9]]
    
     [[10 11 12 13 14]
      [15 16 17 18 19]]
    
     [[20 21 22 23 24]
      [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)
    

There are many ways you might visualize a tensor with more than two axes.


---



3ê°œ ì´ìƒì˜ axesë¥¼ ê°€ì§€ëŠ” í…ì„œë¥¼ í‘œí˜„í•˜ëŠ”ë°ëŠ” ë§ì€ ë°©ë²•ì´ ìˆë‹¤.

![ê·¸ë¦¼2](https://user-images.githubusercontent.com/42956142/135858547-5e18f91d-ddc4-4c6a-bd31-14c8b31b5c20.PNG)

You can convert a tensor to a NumPy array either using np.array or the tensor.numpy method:


---



tensorë¥¼ Numpy ì–´ë ˆì´ë¡œ ë³€í™˜í• ìˆ˜ ìˆë‹¤.

**numpy array ì‚¬ìš©**


```python
np.array(rank_2_tensor)
```




    array([[1., 2.],
           [3., 4.],
           [5., 6.]], dtype=float16)



**numpy method ì‚¬ìš©**


```python
rank_2_tensor.numpy()
```




    array([[1., 2.],
           [3., 4.],
           [5., 6.]], dtype=float16)



Tensors often contain floats and ints, but have many other types, including:

*   complex numbers
*   strings

The base tf.Tensor class requires tensors to be "rectangular"---that is, along each axis, every element is the same size. However, there are specialized types of tensors that can handle different shapes:


*   Ragged tensors (see RaggedTensor below)
*   Sparse tensors (see SparseTensor below)

You can do basic math on tensors, including addition, element-wise multiplication, and matrix multiplication.

------------------------------------------------------------------------------

í…ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë¶€ë™ì†Œìˆ˜ì ê³¼ ì •ìˆ˜í˜•ì„ í¬í•¨í•˜ì§€ë§Œ, ë‹¤ë¥¸ typesì˜ ìë£Œí˜• ì—­ì‹œ í¬í•¨í•œë‹¤.


*   ë³µì†Œìˆ˜
*   ë¬¸ìí˜•

tf.Tensor í´ë˜ìŠ¤ëŠ” í…ì„œê°€ "ì§ì‚¬ê°í˜•"ì´ ë˜ëŠ”ê²ƒì„ ìš”êµ¬í•œë‹¤.
ì¦‰, ê° ì¶•ì„ ë”°ë¼ì„œ, ëª¨ë“  ì›ì†Œê°€ ê°™ì€ í¬ê¸°ë¥¼ ê°€ì ¸ì•¼í•œë‹¤.
ê·¸ëŸ¬ë‚˜ í…ì„œëŠ” ë‹¤ë¥¸ ëª¨ì–‘ì„ ë‹¤ë£° ìˆ˜ ìˆëŠ”, ì§ì‚¬ê°í˜•ì´ ì•„ë‹Œ ê²½ìš°ì—ë„ ì´ë¥¼ ì²˜ë¦¬í• ìˆ˜ ìˆëŠ” íŠ¹ìˆ˜í•œ íƒ€ì…ì„ ê°€ì§€ê³  ìˆë‹¤.

*   ë¹„ì •í˜• í…ì„œ
*   í¬ì†Œ í…ì„œ

í…ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì—°ì‚°ì„ ìˆ˜í–‰í• ìˆ˜ ìˆë‹¤. ë§ì…ˆ, ì›ì†Œë³„ ê³±ì…ˆ
ì™¸ì—ë„ í–‰ë ¬ì˜ ê³±ê¹Œì§€ë„ ê°€ëŠ¥í•˜ë‹¤.


```python
a = tf.constant([[1, 2],
                 [3, 4]])
b = tf.constant([[1, 1],
                 [1, 1]]) # `tf.ones([2,2])`ë˜í•œ ê°€ì§ˆìˆ˜ ìˆë‹¤.

print(tf.add(a, b), "\n")
print(tf.multiply(a, b), "\n")
print(tf.matmul(a, b), "\n")
```

    tf.Tensor(
    [[2 3]
     [4 5]], shape=(2, 2), dtype=int32) 
    
    tf.Tensor(
    [[1 2]
     [3 4]], shape=(2, 2), dtype=int32) 
    
    tf.Tensor(
    [[3 3]
     [7 7]], shape=(2, 2), dtype=int32) 
    
    

**ìˆ˜í•™ ì—°ì‚°**


```python
print(a + b, "\n") # ì›ì†Œí•©
print(a * b, "\n") # ì›ì†Œê³±
print(a @ b, "\n") # í–‰ë ¬ê³±
```

    tf.Tensor(
    [[2 3]
     [4 5]], shape=(2, 2), dtype=int32) 
    
    tf.Tensor(
    [[1 2]
     [3 4]], shape=(2, 2), dtype=int32) 
    
    tf.Tensor(
    [[3 3]
     [7 7]], shape=(2, 2), dtype=int32) 
    
    

Tensors are used in all kinds of operations (ops).

í…ì„œëŠ” ëª¨ë“  ì¢…ë¥˜ì˜ ì—°ì‚°(ops)ì— ì‚¬ìš©ëœë‹¤.



```python
c = tf.constant([[4.0, 5.0], [10.0, 1.0]])

# ìµœëŒ“ê°’ ì°¾ê¸°
print(tf.reduce_max(c))
# ìµœëŒ“ê°’ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
print(tf.argmax(c))
# softmax ì—°ì‚°
print(tf.nn.softmax(c))
```

    tf.Tensor(10.0, shape=(), dtype=float32)
    tf.Tensor([1 0], shape=(2,), dtype=int64)
    tf.Tensor(
    [[2.6894143e-01 7.3105860e-01]
     [9.9987662e-01 1.2339458e-04]], shape=(2, 2), dtype=float32)
    

# About shapes

Tensors have shapes. Some vocabulary:

Shape: The length (number of elements) of each of the axes of a tensor.
Rank: Number of tensor axes. A scalar has rank 0, a vector has rank 1, a matrix is rank 2.
Axis or Dimension: A particular dimension of a tensor.
Size: The total number of items in the tensor, the product shape vector.

Note: Although you may see reference to a "tensor of two dimensions", a rank-2 tensor does not usually describe a 2D space.

---

í…ì„œëŠ” í˜•ìƒì„ ê°–ê³  ìˆìŠµë‹ˆë‹¤. ì“°ì´ëŠ” ìš©ì–´ë¥¼ ì‚´í´ë³´ë©´



*   í˜•ìƒ : í…ì„œì˜ ê° ì¶•ì˜ ê¸¸ì´(ì›ì†Œì˜ ìˆ˜)
*   ë­í¬ : ì¶•ì˜ ê°¯ìˆ˜. ìŠ¤ì¹¼ë¼ëŠ” rank 0, ë²¡í„°ëŠ” rank 1, í–‰ë ¬ì€ rank 2.
*   ì¶• ë˜ëŠ” ì°¨ì› : í…ì„œì˜ íŠ¹ì •í•œ ì°¨ì›
*   ì‚¬ì´ì¦ˆ : í…ì„œ ì•ˆì˜ í•­ëª©ì˜ ì´ ê°¯ìˆ˜. ìƒì„±ëœ í˜•ìƒ ë²¡í„°

**ì£¼ì˜!**

"2ì°¨ì› í…ì„œ"ì— ëŒ€í•œ ì°¸ì¡°ê°€ ìˆì„ìˆ˜ ìˆì§€ë§Œ, rank-2 í…ì„œëŠ” í•­ìƒ 2ì°¨ì› í˜•ìƒì´ ì•„ë‹ˆë‹¤.


Tensors and tf.TensorShape objects have convenient properties for accessing these:

í…ì„œì™€ tf.TensorShapeëŠ” ë‹¤ìŒì„ ì—‘ì„¸ìŠ¤í•˜ê¸°ì— í¸ë¦¬í•œ ì„±ì§ˆì´ ìˆìŠµë‹ˆë‹¤.


```python
rank_4_tensor = tf.zeros([3, 2, 4, 5])
```

![ê·¸ë¦¼ 3](https://user-images.githubusercontent.com/42956142/135858632-5b25447f-bbef-478c-a2d6-06aa049160a5.PNG)


```python
print("ëª¨ë“  ì›ì†Œì˜ íƒ€ì…:", rank_4_tensor.dtype)
print("ì¶•ì˜ ê°¯ìˆ˜:", rank_4_tensor.ndim)
print("í…ì„œì˜ í˜•ìƒ:", rank_4_tensor.shape)
print("0ë²ˆ ì¶•ì˜ ì›ì†Œë“¤:", rank_4_tensor.shape[0])
print("ë§ˆì§€ë§‰ ì¶•ì˜ ì›ì†Œë“¤:", rank_4_tensor.shape[-1])
print("ëª¨ë“  ì›ì†Œì˜ ê°¯ìˆ˜ (3*2*4*5): ", tf.size(rank_4_tensor).numpy())
```

    ëª¨ë“  ì›ì†Œì˜ íƒ€ì…: <dtype: 'float32'>
    ì¶•ì˜ ê°¯ìˆ˜: 4
    í…ì„œì˜ í˜•ìƒ: (3, 2, 4, 5)
    0ë²ˆ ì¶•ì˜ ì›ì†Œë“¤: 3
    ë§ˆì§€ë§‰ ì¶•ì˜ ì›ì†Œë“¤: 5
    ëª¨ë“  ì›ì†Œì˜ ê°¯ìˆ˜ (3*2*4*5):  120
    

While axes are often referred to by their indices, you should always keep track of the meaning of each. Often axes are ordered from global to local: The batch axis first, followed by spatial dimensions, and features for each location last. This way feature vectors are contiguous regions of memory.

ì¶•ì€ ë³´í†µ ì¸ë±ìŠ¤ì— ì˜í•´ ì°¸ì¡°ë˜ëŠ” ë˜ëŠ”ë°, ê·¸ê²ƒë“¤ì´ í•­ìƒ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ íŒŒì•…í•´ì•¼ í•œë‹¤.

ì¶•ë“¤ì€ ìì£¼ ì „ì—­ì—ì„œ ë¡œì»¬ë¡œ ì •ë ¬ëœë‹¤. ë°°ì¹˜ì¶•ì„ ì‹œì‘ìœ¼ë¡œ, ê³µê°„ì°¨ì›, ë§ˆì§€ë§‰ìœ¼ë¡œ ê° ìœ„ì¹˜ì˜ íŠ¹ì„± ìˆœì´ë‹¤. ì´ë¡œì¸í•´, íŠ¹ì„± ë²¡í„°ëŠ” ë©”ëª¨ë¦¬ì— ì—°ì†ì ìœ¼ë¡œ í• ë‹¹ë ìˆ˜ ìˆë‹¤.

![ê·¸ë¦¼4](https://user-images.githubusercontent.com/42956142/135858681-900b7654-ac23-475f-8c85-ac639edd023e.PNG)

# **Indexing**

## Single-axis indexing
TensorFlow follows standard Python indexing rules, similar to indexing a list or a string in Python, and the basic rules for NumPy indexing.

*   indexes start at 0
*   negative indices count backwards from the end
*   colons, :, are used for slices: start:stop:step


---


ë‹¨ì¼ ì¶• ì¸ë±ì‹±

í…ì„œí”Œë¡œìš°ëŠ” íŒŒì´ì¬ì˜ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë¬¸ìì—´ ì¸ë±ì‹±ê³¼ ìœ ì‚¬í•œ í‘œì¤€ íŒŒì´ì¬ ì¸ë±ì‹± ê·œì¹™ê³¼ ë„˜íŒŒì´ ì¸ë±ì‹±ì˜ ê¸°ë³¸ê·œì¹™ì„ ë”°ë¥¸ë‹¤.

*   ì¸ë±ìŠ¤ëŠ” 0ë¶€í„° ì‹œì‘í•œë‹¤.
*   ìŒìˆ˜ ì¸ë±ìŠ¤ëŠ” ë§ˆì§€ë§‰ìœ¼ë¡œë¶€í„° ê±°ê¾¸ë¡œ ì„¼ë‹¤.
*   ":"ì€ ìŠ¬ë¼ì´ì‹±ì— ì‚¬ìš©ëœë‹¤. :start:stop:step


```python
rank_1_tensor = tf.constant([0, 1, 1, 2, 3, 5, 8, 13, 21, 34])
print(rank_1_tensor.numpy())
```

    [ 0  1  1  2  3  5  8 13 21 34]
    

Indexing with a scalar removes the axis:


---

ìŠ¤ì¹¼ë¼ ì¸ë±ì‹±ì€ ì¶•ì„ ì œê±°í•œë‹¤.


```python
print("First:", rank_1_tensor[0].numpy())
print("Second:", rank_1_tensor[1].numpy())
print("Last:", rank_1_tensor[-1].numpy())
```

    First: 0
    Second: 1
    Last: 34
    

Indexing with a : slice keeps the axis:

---

:ì„ ì´ìš©í•œ ì¸ë±ì‹±ì€ ì¶•ì„ ìœ ì§€í•œë‹¤.


```python
print("ëª¨ë“  ì›ì†Œ ì¸ë±ì‹±:", rank_1_tensor[:].numpy())
print("4 ì´ì „ê¹Œì§€ ì¸ë±ì‹±:", rank_1_tensor[:4].numpy())
print("4 ë¶€í„° ëê¹Œì§€ ì¸ë±ì‹±:", rank_1_tensor[4:].numpy())
print("2ë¶€í„° 7ê¹Œì§€ ì¸ë±ì‹±:", rank_1_tensor[2:7].numpy())
print("í™€ìˆ˜ë²ˆì§¸ ì›ì†Œ:", rank_1_tensor[::2].numpy())
print("ê±°ê¾¸ë¡œ:", rank_1_tensor[::-1].numpy())
```

    ëª¨ë“  ì›ì†Œ ì¸ë±ì‹±: [ 0  1  1  2  3  5  8 13 21 34]
    4 ì´ì „ê¹Œì§€ ì¸ë±ì‹±: [0 1 1 2]
    4 ë¶€í„° ëê¹Œì§€ ì¸ë±ì‹±: [ 3  5  8 13 21 34]
    2ë¶€í„° 7ê¹Œì§€ ì¸ë±ì‹±: [1 2 3 5 8]
    í™€ìˆ˜ë²ˆì§¸ ì›ì†Œ: [ 0  1  3  8 21]
    ê±°ê¾¸ë¡œ: [34 21 13  8  5  3  2  1  1  0]
    

## Multi-axis indexing

Higher rank tensors are indexed by passing multiple indices.

The exact same rules as in the single-axis case apply to each axis independently.


---

ë­í¬ê°€ ë†’ì€ í…ì„œëŠ” ì—¬ëŸ¬ ì¸ë±ìŠ¤ë¥¼ ì „ë‹¬í•˜ëŠ”ê²ƒì— ì˜í•´ ì¸ë±ì‹±ì´ ëœë‹¤.

ë‹¨ì¼ì¶•ê³¼ ì •í™•íˆ ê°™ì€ ê·œì¹™ì´ ê° ì¶•ì— ë…ë¦½ì ìœ¼ë¡œ ì ìš©ëœë‹¤.


```python
print(rank_2_tensor.numpy())
```

    [[1. 2.]
     [3. 4.]
     [5. 6.]]
    

Passing an integer for each index, the result is a scalar.

---

ì •ìˆ˜ë¥¼ ê° ì¸ë±ìŠ¤ì— ë„˜ê¸°ë©´ "ìŠ¤ì¹¼ë¼"ê°€ ëœë‹¤.


```python
# Pull out a single value from a 2-rank tensor
print(rank_2_tensor[1, 1].numpy())
```

    4.0
    

You can index using any combination of integers and slices:

---
ì •ìˆ˜ì™€ ":"ì˜ ì¡°í•©ìœ¼ë¡œ ì¸ë±ì‹±ì„ í• ìˆ˜ ìˆë‹¤.


```python
# Get row and column tensors
print("Second row:", rank_2_tensor[1, :].numpy())
print("Second column:", rank_2_tensor[:, 1].numpy())
print("Last row:", rank_2_tensor[-1, :].numpy())
print("First item in last column:", rank_2_tensor[0, -1].numpy())
print("Skip the first row:")
print(rank_2_tensor[1:, :].numpy(), "\n")
```

    Second row: [3. 4.]
    Second column: [2. 4. 6.]
    Last row: [5. 6.]
    First item in last column: 2.0
    Skip the first row:
    [[3. 4.]
     [5. 6.]] 
    
    

Here is an example with a 3-axis tensor:

---
ì¶•ì´ 3ê°œì¸ í…ì„œë¥¼ ì˜ˆë¡œ ë“¤ì–´ë³´ì.



```python
print(rank_3_tensor[:, :, 4])
```

    tf.Tensor(
    [[ 4  9]
     [14 19]
     [24 29]], shape=(3, 2), dtype=int32)
    

![ê·¸ë¦¼ 5](https://user-images.githubusercontent.com/42956142/135858751-99066c7e-8ae4-4c98-a973-4e1cd9291afc.PNG)

# Manipulating Shapes

Reshaping a tensor is of great utility.

---

í…ì„œì˜ í˜•ìƒë³€í™˜ì€ ë§¤ìš° í¸ë¦¬í•˜ë‹¤.


```python
# í˜•ìƒì€ ì¶•ì„ ë”°ë¼ ì‚¬ì´ì¦ˆë¥¼ ë³´ì—¬ì£¼ëŠ 'TensorShape'ì˜¤ë¸Œì íŠ¸ë¥¼ ë¦¬í„´í•œë‹¤.
x = tf.constant([[1], [2], [3]])
print(x.shape)
```

    (3, 1)
    


```python
# x.shapeì˜ íƒ€ì…ì„ í™•ì¸í•´ë³´ì
print(type(x.shape))
```

    <class 'tensorflow.python.framework.tensor_shape.TensorShape'>
    


```python
# ë˜í•œ ì´ 'TensorShape'ì˜¤ë¸Œì íŠ¸ë¥¼ íŒŒì´ì¬ë¦¬ìŠ¤íŠ¸ì— ì§‘ì–´ë„£ì„ìˆ˜ ìˆë‹¤.
print(x.shape.as_list())
```

    [3, 1]
    

You can reshape a tensor into a new shape. The tf.reshape operation is fast and cheap as the underlying data does not need to be duplicated.

---

ìƒˆë¡œìš´ í˜•ìƒìœ¼ë¡œ í…ì„œë¥¼ ë³€í™˜ í• ìˆ˜ìˆë‹¤.

tf.reshape ì—°ì‚°ì€ ê¸°ë³¸ ë°ì´í„°ê°€ ë³µì‚¬ë  í•„ìš”ê°€ ì—†ê¸° ë•Œë¬¸ì— ë¹ ë¥´ê³  ê°€ë³ë‹¤.


```python
# ìƒˆë¡œìš´ í˜•ìƒì— í…ì„œë¥¼ ë³€í™˜í• ìˆ˜ ìˆë‹¤.
# ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬í•˜ëŠ” ê²ƒì„ ì£¼ëª©í•´ë¼
reshaped = tf.reshape(x, [1, 3])
```


```python
print(x.shape)
print(reshaped.shape)
```

    (3, 1)
    (1, 3)
    

The data maintains its layout in memory and a new tensor is created, with the requested shape, pointing to the same data. TensorFlow uses C-style "row-major" memory ordering, where incrementing the rightmost index corresponds to a single step in memory.

---

ë°ì´í„°ëŠ” ë©”ëª¨ë¦¬ì—ì„œ ë ˆì´ì•„ì›ƒì„ ìœ ì§€í•˜ê³ , ê°™ì€ ë°ì´í„°ë¥¼ ê°€ë¦¬í‚¤ëŠ” ìƒˆë¡œìš´ í…ì„œê°€ ìš”ì²­ëœ í˜•ìƒìœ¼ë¡œ ë§Œë“¤ì–´ì§„ë‹¤.

í…ì„œí”Œë¡œìš°ëŠ” C-ìŠ¤íƒ€ì¼ì˜ "í–‰ ìš°ì„ " ë©”ëª¨ë¦¬ ìˆœì„œë¥¼ ì‚¬ìš©í•˜ë©°, ê°€ì¥ ì˜¤ë¥¸ìª½ ì¸ë±ìŠ¤ì˜ ì¦ê°€ëŠ” ë©”ëª¨ë¦¬ì˜ 1ìŠ¤í…ì— í•´ë‹¹í•œë‹¤.


```python
print(rank_3_tensor)
```

    tf.Tensor(
    [[[ 0  1  2  3  4]
      [ 5  6  7  8  9]]
    
     [[10 11 12 13 14]
      [15 16 17 18 19]]
    
     [[20 21 22 23 24]
      [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)
    

If you flatten a tensor you can see what order it is laid out in memory.

---

ë©”ëª¨ë¦¬ì— ì–´ë–¤ ìˆœì„œë¡œ ë†“ì—¬ì ¸ ìˆëŠ”ì§€ëŠ” í…ì„œë¥¼ í‰í‰í•˜ê²Œ í•˜ë©´, ì¦‰ 1ì°¨ì›ìœ¼ë¡œ ë§Œë“¤ë©´ í™•ì¸í• ìˆ˜ ìˆë‹¤.


```python
# "í˜•ìƒ" argumentì— ì „ë‹¬ëœ "-1"ì€ "í•´ë‹¹í•˜ëŠ” ë¬´ì—‡ì´ë“ ì§€"ë¥¼ ëœ»í•œë‹¤. 
print(tf.reshape(rank_3_tensor, [-1]))
```

    tf.Tensor(
    [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
     24 25 26 27 28 29], shape=(30,), dtype=int32)
    

Typically the only reasonable use of tf.reshape is to combine or split adjacent axes (or add/remove 1s).

For this 3x2x5 tensor, reshaping to (3x2)x5 or 3x(2x5) are both reasonable things to do, as the slices do not mix:

---

ì¼ë°˜ì ìœ¼ë¡œ tf.reshapeì˜ ìœ ì¼í•œ í•©ë¦¬ì ì¸ ì‚¬ìš©ì€ ì¸ì ‘í•œ ì¶•ì˜ ê²°í•© or ë¶„ë¦¬ì´ë‹¤.(ë˜ëŠ” 1ì˜ ì¶”ê°€/ì œê±°)

3x2x5 í…ì„œë¥¼ ì˜ˆë¡œë“¤ë©´, (3x2)x5 ë˜ëŠ” 3x(2x5)ë¡œì˜ ë³€í™˜ì€ ë‘˜ë‹¤ ìŠ¬ë¼ì´ìŠ¤ê°€ ì„ì´ì§€ ì•Šìœ¼ë¯€ë¡œ, í•©ë¦¬ì ì´ë‹¤.


```python
print(tf.reshape(rank_3_tensor, [3*2, 5]), "\n")
print(tf.reshape(rank_3_tensor, [3, -1]))
```

    tf.Tensor(
    [[ 0  1  2  3  4]
     [ 5  6  7  8  9]
     [10 11 12 13 14]
     [15 16 17 18 19]
     [20 21 22 23 24]
     [25 26 27 28 29]], shape=(6, 5), dtype=int32) 
    
    tf.Tensor(
    [[ 0  1  2  3  4  5  6  7  8  9]
     [10 11 12 13 14 15 16 17 18 19]
     [20 21 22 23 24 25 26 27 28 29]], shape=(3, 10), dtype=int32)
    

![ê·¸ë¦¼ 6](https://user-images.githubusercontent.com/42956142/135858837-5c36fe6d-ad61-4dab-a91c-8e8b6fe0e1c5.PNG)

Reshaping will "work" for any new shape with the same total number of elements, but it will not do anything useful if you do not respect the order of the axes.

Swapping axes in tf.reshape does not work; you need tf.transpose for that.

---

í˜•ìƒë³€í™˜ì€ ì–´ë– í•œ ìƒˆë¡œìš´ í˜•ìƒì— ëŒ€í•´ì„œë„ ê°™ì€ ì›ì†Œê°¯ìˆ˜ë¥¼ ê°€ì§€ê³  ì‘ë™í•  ê²ƒì´ë‹¤.
ê·¸ëŸ¬ë‚˜ ì¶•ì˜ ìˆœì„œë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´, ì“¸ëª¨ìˆì§€ëŠ” ì•Šì„ê²ƒì´ë‹¤.

tf.reshape ì•ˆì—ì„œì˜ ì¶• êµí™˜ì€ ë¶ˆê°€ëŠ¥í•˜ë‹¤. ì´ ê²½ìš°ì—” tf.transposeê°€ í•„ìš”í•˜ë‹¤.


```python
# Bad examples: don't do this

# You can't reorder axes with reshape.
print(tf.reshape(rank_3_tensor, [2, 3, 5]), "\n") 

# This is a mess
print(tf.reshape(rank_3_tensor, [5, 6]), "\n")

# This doesn't work at all
try:
  tf.reshape(rank_3_tensor, [7, -1])
except Exception as e:
  print(f"{type(e).__name__}: {e}")
```

    tf.Tensor(
    [[[ 0  1  2  3  4]
      [ 5  6  7  8  9]
      [10 11 12 13 14]]
    
     [[15 16 17 18 19]
      [20 21 22 23 24]
      [25 26 27 28 29]]], shape=(2, 3, 5), dtype=int32) 
    
    tf.Tensor(
    [[ 0  1  2  3  4  5]
     [ 6  7  8  9 10 11]
     [12 13 14 15 16 17]
     [18 19 20 21 22 23]
     [24 25 26 27 28 29]], shape=(5, 6), dtype=int32) 
    
    InvalidArgumentError: Input to reshape is a tensor with 30 values, but the requested shape requires a multiple of 7 [Op:Reshape]
    

![ê·¸ë¦¼ 7](https://user-images.githubusercontent.com/42956142/135858901-f736b56f-0e76-4457-a310-e5e507ceee02.PNG)

You may run across not-fully-specified shapes. Either the shape contains a None (an axis-length is unknown) or the whole shape is None (the rank of the tensor is unknown).

Except for tf.RaggedTensor, such shapes will only occur in the context of TensorFlow's symbolic, graph-building APIs:

*   tf.function
*   The keras functional API.


---

ì¼ë¶€ê°€ íŠ¹ì •ë˜ì§€ ì•Šì€ í˜•ìƒì„ ì‹¤í–‰í• ë•Œë„ ìˆì„ê²ƒì´ë‹¤.í˜•ìƒì´ None(ì¶•ì˜ ê¸¸ì´ê°€ unknwon)ì„ í¬í•¨í•˜ê±°ë‚˜ ë˜ëŠ” í˜•ìƒ ì „ì²´ê°€ None, ì¦‰ í…ì„œì˜ rankê°€ unkownì¸ ê²½ìš°ë„ ìˆì„ê²ƒì´ë‹¤.

tf.RaggedTensrì„ ì œì™¸í•˜ê³ , ì´ëŸ¬í•œ í˜•ìƒë“¤ì€ TensorFlowì˜ ìƒì§•ì ì¸ graph-builiding APIsì—ì„œ ë°œìƒí•  ìˆ˜ ìˆë‹¤.

*   tf.function
*   The keras functional API.



# More on DTypes

To inspect a tf.Tensor's data type use the Tensor.dtype property.

When creating a tf.Tensor from a Python object you may optionally specify the datatype.

If you don't, TensorFlow chooses a datatype that can represent your data. TensorFlow converts Python integers to tf.int32 and Python floating point numbers to tf.float32. Otherwise TensorFlow uses the same rules NumPy uses when converting to arrays.

You can cast from type to type.

---
tf.Tensorì˜ ë°ì´í„° íƒ€ì…ì„ ë“¤ì—¬ë‹¤ ë³´ê¸°ìœ„í•´ Tensor.dtype ì†ì„±ì„ ì‚¬ìš©í•œë‹¤.

íŒŒì´ì¬ ê°ì²´ë¡œë¶€í„° tf.Tensorì„ ìƒì„±í• ë•Œ, ì˜µì…˜ìœ¼ë¡œ ë°ì´í„°íƒ€ì…ì„ ì§€ì •í• ìˆ˜ ìˆë‹¤.

ë§Œì•½ ê·¸ë ‡ì§€ ì•ŠëŠ”ë‹¤ë©´, í…ì„œí”Œë¡œìš°ëŠ” ë°ì´í„°ë¥¼ í‘œí˜„í• ìˆ˜ ìˆëŠ” ë°ì´í„°íƒ€ì…ì„ ì„ íƒí•œë‹¤.
í…ì„œí”Œë¡œìš°ëŠ” íŒŒì´ì¬ integersë¥¼ tf.int32ë¡œ ë°”ê¾¸ê³ , íŒŒì´ì¬ ì‹¤ìˆ˜ë¥¼ tf.float32ìœ¼ë¡œ ë°”ê¾¼ë‹¤. ê·¸ì™¸ì—ë„ í…ì„œí”Œë¡œìš°ëŠ” ì–´ë ˆì´ë¥¼ ë³€í™˜í• ë•Œ Numpyì™€ ê°™ì€ ê·œì¹™ì„ ì‚¬ìš©í•œë‹¤.


```python
the_f64_tensor = tf.constant([2.2, 3.3, 4.4], dtype=tf.float64)
the_f16_tensor = tf.cast(the_f64_tensor, dtype=tf.float16)
# Now, cast to an uint8 and lose the decimal precision
the_u8_tensor = tf.cast(the_f16_tensor, dtype=tf.uint8)
print(the_u8_tensor)
```

    tf.Tensor([2 3 4], shape=(3,), dtype=uint8)
    

# Broadcasting

Broadcasting is a concept borrowed from the equivalent feature in NumPy. In short, under certain conditions, smaller tensors are "stretched" automatically to fit larger tensors when running combined operations on them.

The simplest and most common case is when you attempt to multiply or add a tensor to a scalar. In that case, the scalar is broadcast to be the same shape as the other argument.

---

"ë¸Œë¡œë“œìºìŠ¤íŒ…"ì€ ë„˜íŒŒì´ì˜ íŠ¹ì„±ìœ¼ë¡œ ë¶€í„° ê°€ì ¸ì˜¨ ê°œë…ì´ë‹¤.
íŠ¹ì •í•œ ìƒíƒœì—ì„œ, ê²°í•© ì—°ì‚°ì„ ìˆ˜í–‰í• ë•Œ, ì‘ì€ í…ì„œëŠ” ë” í° í…ì„œì— ë§ì¶° ìë™ì ìœ¼ë¡œ "í™•ì¥"ëœë‹¤.

ì´ ë¸Œë¡œë“œ ìºìŠ¤íŒ…ì˜ ê°€ì¥ ê°„ë‹¨í•˜ê³  ì¼ë°˜ì ì¸ ê²½ìš°ëŠ” í…ì„œë¥¼ ìŠ¤ì¹¼ë¼ì— ê³±ì´ë‚˜ ë§ì…ˆì—°ì‚°ì„ í• ë•Œì´ë‹¤. ì´ëŸ¬í•œ ê²½ìš° ìŠ¤ì¹¼ë¼ëŠ” ë‹¤ë¥¸ argumentì™€ ê°™ì€ í˜•ìƒì´ ë˜ë„ë¡ ë¸Œë¡œë“œìºìŠ¤ë“œ ëœë‹¤.




```python
x = tf.constant([1, 2, 3])

y = tf.constant(2)
z = tf.constant([2, 2, 2])
#ëª¨ë‘ ê°™ì€ ì—°ì‚°ì´ë‹¤.
print(tf.multiply(x, 2))
print(x * y)
print(x * z)
```

    tf.Tensor([2 4 6], shape=(3,), dtype=int32)
    tf.Tensor([2 4 6], shape=(3,), dtype=int32)
    tf.Tensor([2 4 6], shape=(3,), dtype=int32)
    

Likewise, axes with length 1 can be stretched out to match the other arguments. Both arguments can be stretched in the same computation.

In this case a 3x1 matrix is element-wise multiplied by a 1x4 matrix to produce a 3x4 matrix. Note how the leading 1 is optional: The shape of y is [4].

---

ì´ì™€ ê°™ì´, ê¸¸ì´ê°€ 1ì¸ ì¶•ì€ ë‹¤ë¥¸ argumentì™€ ë§¤ì¹˜ë ìˆ˜ ìˆë„ë¡ í™•ì¥ëœë‹¤.
ì–‘ìª½ì˜ argumentsëŠ” ê°™ì€ ê³„ì‚°ìœ¼ë¡œ í™•ì¥ë ìˆ˜ ìˆë‹¤.

3x1 í–‰ë ¬ì˜ ê²½ìš°, 1x4 í–‰ë ¬ì˜ element-wiseê³±ì—°ì‚°ì€ 3x4í–‰ë ¬ì„ ë§Œë“¤ì–´ë‚¸ë‹¤.
ì•ì˜ "1"ì´ ì˜µì…˜ì¸ê²ƒì— ì£¼ëª©í•˜ì. yì˜ í˜•ìƒì€ [4]ì´ë‹¤.


```python
#ëª¨ë‘ ê°™ì€ì—°ì‚°ì´ë‹¤
x = tf.reshape(x,[3,1])
y = tf.range(1, 5)
print(x, "\n")
print(y, "\n")
print(tf.multiply(x, y))
```

    tf.Tensor(
    [[1]
     [2]
     [3]], shape=(3, 1), dtype=int32) 
    
    tf.Tensor([1 2 3 4], shape=(4,), dtype=int32) 
    
    tf.Tensor(
    [[ 1  2  3  4]
     [ 2  4  6  8]
     [ 3  6  9 12]], shape=(3, 4), dtype=int32)
    

![ê·¸ë¦¼ 8](https://user-images.githubusercontent.com/42956142/135858938-2cb1f452-6896-4145-b7dd-815718cd8952.PNG)

ë¸Œë¡œë“œìºìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ ë„ ìœ„ì™€ ê°™ì€ ì—°ì‚°ì„ í• ìˆ˜ ìˆë‹¤.


```python
x_stretch = tf.constant([[1, 1, 1, 1],
                         [2, 2, 2, 2],
                         [3, 3, 3, 3]])

y_stretch = tf.constant([[1, 2, 3, 4],
                         [1, 2, 3, 4],
                         [1, 2, 3, 4]])

print(x_stretch * y_stretch)  # Again, operator overloading
```

    tf.Tensor(
    [[ 1  2  3  4]
     [ 2  4  6  8]
     [ 3  6  9 12]], shape=(3, 4), dtype=int32)
    

Most of the time, broadcasting is both time and space efficient, as the broadcast operation never materializes the expanded tensors in memory.

You see what broadcasting looks like using tf.broadcast_to.

---

ëŒ€ë¶€ë¶„ì˜ ê²½ìš°, ë¸Œë¡œë“œìºìŠ¤íŒ…ì€ ì‹œê°„ê³¼ ê³µê°„ì´ íš¨ìœ¨ì ì¸ë° ì´ëŠ” ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì—°ì‚°ì´ ë©”ëª¨ë¦¬ì—ì„œ í™•ì¥ëœ í…ì„œë¥¼ ì‹¤ì²´í™” í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤.


```python
print(tf.broadcast_to(tf.constant([1, 2, 3]), [3, 3]))
```

    tf.Tensor(
    [[1 2 3]
     [1 2 3]
     [1 2 3]], shape=(3, 3), dtype=int32)
    

Unlike a mathematical op, for example, broadcast_to does nothing special to save memory. Here, you are materializing the tensor.

It can get even more complicated. This section of Jake VanderPlas's book Python Data Science Handbook shows more broadcasting tricks (again in NumPy).

---

ì˜ˆë¥¼ë“¤ë©´ ìˆ˜í•™ì ì¸ ì—°ì‚°ê³¼ëŠ” ë‹¤ë¥´ê²Œ, ë¸Œë¡œë“œìºìŠ¤íŠ¸ëŠ” ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•˜ê¸°ìœ„í•œ ì–´ë– í•œ íŠ¹ë³„í•œê²ƒë„ í•˜ì§€ ì•ŠëŠ”ë‹¤. ì—¬ê¸°ì— í…ì„œë¥¼ ì‹¤ì²´í™”í•œë‹¤.

ê±°ê¸°ì— ë” ë³µì¡í•´ì§ˆìˆ˜ ìˆë‹¤.

# tf.convert_to_tensor

Most ops, like tf.matmul and tf.reshape take arguments of class tf.Tensor. However, you'll notice in the above case, Python objects shaped like tensors are accepted.

Most, but not all, ops call convert_to_tensor on non-tensor arguments. There is a registry of conversions, and most object classes like NumPy's ndarray, TensorShape, Python lists, and tf.Variable will all convert automatically.

See tf.register_tensor_conversion_function for more details, and if you have your own type you'd like to automatically convert to a tensor.

---
tf.matmulê³¼ tf.reshapeì™€ ê°™ì€ ëŒ€ë¶€ë¶„ì˜ ì—°ì‚°ì€ tf.Tensor í´ë˜ìŠ¤ì˜ ì¸ìˆ˜ë“¤ì„ ì·¨ê¸‰í•˜ì§€ë§Œ, ìœ„ì˜ ê²½ìš°ë¥¼ ìƒê°í•´ë³´ë©´, í…ì„œì™€ ê°™ì´ í˜•ìƒ íŒŒì´ì¬ ê°ì²´ëŠ” ë„˜ê¸°ëŠ”ê²½ìš°ê°€ ìˆë‹¤.

ì „ë¶€ëŠ” ì•„ë‹ˆì§€ë§Œ, ëŒ€ë¶€ë¶„ ì—°ì‚°ì€ ë¹„í…ì„œ ì¸ìˆ˜ì— ëŒ€í•´ì„œ convert_to_tensor í˜¹ì€ non_tesnsor argumentsë¥¼ í˜¸ì¶œí•œë‹¤. ë³€í™˜ì˜ ë ˆì§€ìŠ¤íŠ¸ë¦¬ê°€ ìˆê³ , NumPyì˜ ndarray, TensorShape, Python lists, ê·¸ë¦¬ê³  tf.Variableê³¼ ê°™ì€ ë§ì€ ê°ì²´ í´ë˜ìŠ¤ë“¤ì€ ìë™ì ìœ¼ë¡œ ë³€í™˜ë  ê²ƒì´ë‹¤.


# Ragged Tensors

A tensor with variable numbers of elements along some axis is called "ragged". Use tf.ragged.RaggedTensor for ragged data.

For example, This cannot be represented as a regular tensor:

---
ì¶•ì— ë§ì¶° ìš”ì†Œì˜ ìˆ˜ê°€ ë³€í•˜ëŠ” í…ì„œë¥¼ "ë¹„ì •í˜•"ì´ë¼ ë¶€ë¥¸ë‹¤.
ë¹„ì •í˜• ë°ì´í„°ì— tf.ragged.RaggedTensorë¥¼ ì¨ë³´ì.

ì˜ˆë¥¼ë“¤ì–´ ë‹¤ìŒì˜ ê²½ìš°ëŠ” ì •ê·œ í…ì„œë¡œ ëŒ€í‘œë ìˆ˜ ì—†ë‹¤.



```python
ragged_list = [
    [0, 1, 2, 3],
    [4, 5],
    [6, 7, 8],
    [9]]
```

![ê·¸ë¦¼ 9](https://user-images.githubusercontent.com/42956142/135858996-13a61f37-75c2-4617-93b8-83b10c79ed39.PNG)


```python
try:
  tensor = tf.constant(ragged_list)
except Exception as e:
  print(f"{type(e).__name__}: {e}")

```

    ValueError: Can't convert non-rectangular Python sequence to Tensor.
    

Instead create a tf.RaggedTensor using tf.ragged.constant:

---

ëŒ€ì‹ , tf.ragged.constantë¥¼ ì‚¬ìš©í•´ tf.RaggedTensorë¥¼ ìƒì„±í•´ë³´ì.


```python
ragged_tensor = tf.ragged.constant(ragged_list)
print(ragged_tensor)
```

    <tf.RaggedTensor [[0, 1, 2, 3], [4, 5], [6, 7, 8], [9]]>
    

The shape of a tf.RaggedTensor will contain some axes with unknown lengths:

---

tf.RaggedTensorì˜ í˜•ìƒì€ ê¸¸ì´ê°€ ë¶ˆëª…í™•í•œ ì¶•ì„ í¬í•¨í• ê²ƒì´ë‹¤.



```python
print(ragged_tensor.shape)
```

    (4, None)
    

## String tensors
tf.string is a dtype, which is to say you can represent data as strings (variable-length byte arrays) in tensors.

The strings are atomic and cannot be indexed the way Python strings are. The length of the string is not one of the axes of the tensor. See tf.strings for functions to manipulate them.

Here is a scalar string tensor:

---
tf.stringì€ í…ì„œì—ì„œ ë°ì´í„°ë¥¼ ë¬¸ìì—´(ê¸¸ì´-ê°€ë³€ ë°”ì´íŠ¸ ë°°ì—´)ë¡œ í‘œí˜„í• ìˆ˜ ìˆëŠ” dtypeì´ë‹¤.

ë¬¸ìì—´ì€ atomicí•˜ê³  íŒŒì´ì¬ ë¬¸ìì—´ê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì¸ë±ì‹± ë ìˆ˜ ì—†ë‹¤.
ë¬¸ìì˜ ê¸¸ì´ëŠ” í…ì„œì˜ í•˜ë‚˜ì˜ ì¶•ì´ ì•„ë‹ˆë‹¤.

ë‹¤ìŒì€ ìŠ¤ì¹¼ë¼ ë¬¸ìì—´ í…ì„œì´ë‹¤.


```python
# Tensors can be strings, too here is a scalar string.
scalar_string_tensor = tf.constant("Gray wolf")
print(scalar_string_tensor)
```

    tf.Tensor(b'Gray wolf', shape=(), dtype=string)
    

ìŠ¤íŠ¸ë§ì˜ ë²¡í„°ëŠ”

![ê·¸ë¦¼ 10](https://user-images.githubusercontent.com/42956142/135859052-8e56769c-3065-4e19-bb91-be4f5727ab03.PNG)

```python
# ê¸¸ì´ê°€ ë‹¤ë¥¸ 3ê°œì˜ ë¬¸ìì—´ í…ì„œëŠ” ê´œì°®ë‹¤.
tensor_of_strings = tf.constant(["Gray wolf",
                                 "Quick brown fox",
                                 "Lazy dog"])
# í˜•ìƒì´(3,)ì¸ê²ƒì— ì£¼ëª©í•˜ì. ë¬¸ìì—´ê¸¸ì´ëŠ” í¬í•¨ë˜ì§€ ì•Šì•˜ë‹¤.
print(tensor_of_strings)
```

    tf.Tensor([b'Gray wolf' b'Quick brown fox' b'Lazy dog'], shape=(3,), dtype=string)
    

In the above printout the b prefix indicates that tf.string dtype is not a unicode string, but a byte-string. See the Unicode Tutorial for more about working with unicode text in TensorFlow.

If you pass unicode characters they are utf-8 encoded.

---

ìœ„ì˜ ì¶œë ¥ì—ì„œ, ì ‘ë‘ì‚¬ bëŠ” tf.strying dtypeì´ ìœ ë‹ˆì½”ë“œ ë¬¸ìì—´ì´ ì•„ë‹ˆë¼, ë°”ì´íŠ¸ ë¬¸ìì—´ì„ ë§í•œë‹¤. 

ë§Œì•½ ìœ ë‹ˆì½”ë“œ ë¬¸ìë¥¼ ì „ë‹¬í•´ì¤„ê²½ìš°, utf-8ë¡œ ì¸ì½”ë“œëœë‹¤.


```python
tf.constant("ğŸ¥³ğŸ‘")
```




    <tf.Tensor: shape=(), dtype=string, numpy=b'\xf0\x9f\xa5\xb3\xf0\x9f\x91\x8d'>



Some basic functions with strings can be found in tf.strings, including tf.strings.split.

---
tf.sringsì™€ tf.srings.splitì—ëŠ” ë¬¸ìì—´ì— ê´€í•œ ê¸°ë³¸ì ì¸ í•¨ìˆ˜ê°€ ë‹´ê²¨ì ¸ìˆë‹¤.



```python
# ë¬¸ìì—´ì„ í…ì„œì˜ ì§‘í•©ìœ¼ë¡œ ë¶„í• í•˜ê³  ì‹¶ì€ê²½ìš° splitì„ ì‚¬ìš©í• ìˆ˜ ìˆë‹¤.
print(tf.strings.split(scalar_string_tensor, sep=" "))
```

    tf.Tensor([b'Gray' b'wolf'], shape=(2,), dtype=string)
    


```python
#ê·¸ëŸ¬ë‚˜ ë¬¸ìì—´í…ì„œë¥¼ ë¶„í• í• ê²½ìš°, "ë¹„ì •í˜• í…ì„œ"ê°€ ë˜ëŠ”ë°
# ê° ë¬¸ìì—´ì€ ë‹¤ë¥¸ ì˜ì—­ìœ¼ë¡œ ë¶„í• ë  ê²ƒì´ë‹¤.
print(tf.strings.split(tensor_of_strings))
```

    <tf.RaggedTensor [[b'Gray', b'wolf'], [b'Quick', b'brown', b'fox'], [b'Lazy', b'dog']]>
    

![ê·¸ë¦¼ 12](https://user-images.githubusercontent.com/42956142/135859496-d60aa4b4-2719-4155-ae20-a7a962ecab1d.PNG)


And tf.string.to_number:


```python
text = tf.constant("1 10 100")
print(tf.strings.to_number(tf.strings.split(text, " ")))
```

    tf.Tensor([  1.  10. 100.], shape=(3,), dtype=float32)
    

Although you can't use tf.cast to turn a string tensor into numbers, you can convert it into bytes, and then into numbers.

---
tf.castë¥¼ ì‚¬ìš©í•´ ë¬¸ìì—´í…ì„œë¥¼ ìˆ«ìë¡œ ë°”ê¿€ìˆœ ì—†ì§€ë§Œ, ì´ë¥¼ ë°”ì´íŠ¸ë¡œ ë°”ê¾¼ë’¤, ë‹¤ì‹œ ìˆ«ìë¡œ ë°”ê¿€ìˆœ ìˆë‹¤.



```python
byte_strings = tf.strings.bytes_split(tf.constant("Duck"))
byte_ints = tf.io.decode_raw(tf.constant("Duck"), tf.uint8)
print("Byte strings:", byte_strings)
print("Bytes:", byte_ints)
```

    Byte strings: tf.Tensor([b'D' b'u' b'c' b'k'], shape=(4,), dtype=string)
    Bytes: tf.Tensor([ 68 117  99 107], shape=(4,), dtype=uint8)
    


```python
# Or split it up as unicode and then decode it
unicode_bytes = tf.constant("ã‚¢ãƒ’ãƒ« ğŸ¦†")
unicode_char_bytes = tf.strings.unicode_split(unicode_bytes, "UTF-8")
unicode_values = tf.strings.unicode_decode(unicode_bytes, "UTF-8")

print("\nUnicode bytes:", unicode_bytes)
print("\nUnicode chars:", unicode_char_bytes)
print("\nUnicode values:", unicode_values)
```

    
    Unicode bytes: tf.Tensor(b'\xe3\x82\xa2\xe3\x83\x92\xe3\x83\xab \xf0\x9f\xa6\x86', shape=(), dtype=string)
    
    Unicode chars: tf.Tensor([b'\xe3\x82\xa2' b'\xe3\x83\x92' b'\xe3\x83\xab' b' ' b'\xf0\x9f\xa6\x86'], shape=(5,), dtype=string)
    
    Unicode values: tf.Tensor([ 12450  12498  12523     32 129414], shape=(5,), dtype=int32)
    

The tf.string dtype is used for all raw bytes data in TensorFlow. The tf.io module contains functions for converting data to and from bytes, including decoding images and parsing csv.



---
tf.string dtypeëŠ” í…ì„œí”Œë¡œìš° ì•ˆì˜ ëª¨ë“  raw bytes dateì— ì‚¬ìš©ëœë‹¤. 

tf.io ëª¨ë“ˆì€ ì´ë¯¸ì§€ì˜ í•´ë…ê³¼ csv ë¶„ì„ ë“± ë°ì´í„°ì™€ ë°”ì´íŠ¸ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ í¬í•¨í•œë‹¤.

# Sparse tensors
Sometimes, your data is sparse, like a very wide embedding space. TensorFlow supports tf.sparse.SparseTensor and related operations to store sparse data efficiently.

---

ê°€ë”, ê³µê°„ì´ ë§¤ìš° ì»¤ì„œ ë°ì´í„°ê°€ ê²°ì¸¡ê°’ì´ ë§ì„ë•Œë„ ìˆë‹¤. í…ì„œí”Œë¡œìš°ëŠ” ë¹ˆê³µê°„ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•˜ê¸° ìœ„í•´, tf.sparse.SparseTensorì™€ ê´€ë ¨ëœ ê¸°ëŠ¥ë“¤ì„ ì œê³µí•œë‹¤.

![ê·¸ë¦¼ 13](https://user-images.githubusercontent.com/42956142/135859532-215f6697-b1d9-4948-98da-f0b5f8aeeab4.PNG)


```python
# í¬ì†Œ í…ì„œëŠ” ì¸ë±ìŠ¤ì˜ ê°’ë“¤ì„ ë©”ëª¨ë¦¬ì— íš¨ìœ¨ì ì¸ ë°©ì‹ìœ¼ë¡œ ì €ì¥í•œë‹¤.
sparse_tensor = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]],
                                       values=[1, 2],
                                       dense_shape=[3, 4])
print(sparse_tensor, "\n")

# You can convert sparse tensors to dense
print(tf.sparse.to_dense(sparse_tensor))
```

    SparseTensor(indices=tf.Tensor(
    [[0 0]
     [1 2]], shape=(2, 2), dtype=int64), values=tf.Tensor([1 2], shape=(2,), dtype=int32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64)) 
    
    tf.Tensor(
    [[1 0 0 0]
     [0 0 2 0]
     [0 0 0 0]], shape=(3, 4), dtype=int32)
    
